\documentclass[11pt,a4paper]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{multirow}
\usepackage{subcaption}
\captionsetup{justification=centering}
\titleformat*{\section}{\large\bfseries}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{array}

    \makeatletter
\newcommand{\thickhline}{%
	\noalign {\ifnum 0=`}\fi \hrule height 1pt
	\futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\vrule width 1pt}}
\makeatother



\begin{document}
	
	\title{\textbf{Master Project \\ Computer Graphics: Rendering Track}}
	\author{\textbf{Alhajras Algdairy} \\ \textbf{Advisor: Prof. 
			Dr.-Ing. Matthias Teschner}}
	\maketitle
	
	\section*{\centering Acknowledgment}
	I want to express my special thanks of gratitude to Prof. Dr.-Ing. Matthias Teschner, who guided me through my master's program in general and in this master's project in specific. This project gave me the golden opportunity to dig into an exciting topic in computer graphics that I am keen on. The flexibility in research made me learn more about the related topics I am interested in, in rendering. Reading scientific topics and reviewing codes from different repositories expanded my expertise, where I had a full responsibility to control my time and resources, to learn more about the research process. 	
	
	\section*{\centering Abstract}
	This report investigates four different acceleration data-structure methods for implementing a simple raytracer on CPU. These methods are KD-tree, Uniformgrid, BVH, and LBVH. The report aims to compare the different approaches and their impact on the raytracer rendering time. The raytracer used in this project builds upon a previous lab project where the foundation of the raytracer is already implemented; however, without using a sophisticated data-structure to enhance the rending performance; hence the focus of this report will be how to improve the raytracer performance by using data-structures. 
	\textbf{\textit{Keywords: Ray Tracing; kD-Tree; BVH; LBVH; Uniformgird; Data structure;}}
	
	\section{Introduction}
	This report summarises my journey to implement a simple raytracer focusing on data structure level, where the performance of rending a scene will be the lion's share of the report. Theory, implementation, and results will be discussed in depth in the report, where an introduction and motivation of what is raytracer and how to implement it will be briefly discussed; because data structures are more interesting for us, I will only explain the topics that are used in this raytracer. 
	
	\section{What is Raytrcer and how it works}
	
	Computer graphics has three main pillars: \textit{Modelling}, \textit{Rendering}, and \textit{Simulation}. Scientists are interested in simulating a real-life phenomenon, such as Gravity Fraction, Rain, Snow, and the exciting part for us, light.  Simulating light is arguably the most challenging part because light has always been difficult to characterize as it can behave as particles and waves, which makes it spread into the whole scene based on probabilities. This makes it difficult to compute as it involves complex computation and infinite simulations to execute to have a perfect result without an error, this is what is known as Rendering.
	
	Why do we need to simulate light? Adding light into a scene will generate shadows, reflection, and refraction consequently will illuminate the scene, making the scene look like a reality, which can be helpful for some applications. For the internal designers, it is vital to simulate the final result of the lightning inside a room, such as sunlight coming from the windows, the light of lamps, and fireplace, these with react together and create shadows, before constructing the building and payloads of money, simulating the right angle and place of the room is helpful to imagine the final result. Solar engineers use 3D tools to build solar panel farms where the angle between the sunlight and the panel is essential to gather as much light energy as possible. In gaming, different companies compete to design engines that can produce natural scenes; this includes lighting and shadowing.  
	
	Figure~\ref{fig:1}, shows an example of rending a scene that can not be distinguished from reality, the details it catches as glossy materials, the reflection of surfaces, and the shadow. Capturing these details requires rendering techniques. Two popular methods are Rasterization and Raytracing. Raytracing outperforms rasterization in capturing more details; however this leads to performance issues, that is why most application uses Raytracing is offline rendering and not real-time rendering like games, where speed is vital to render each frame with compromising details. 
	
	
		\begin{figure}[H]
		\begin{center}
		\includegraphics[width=0.7\textwidth]{1.jpg}
			
			\caption{https://www.pcgamer.com/unreal-engine-5-tech-demo-pc-performance/ The raytracing algorithm builds an image by extending rays into a scene and bouncing them off surfaces and towards sources of light to approximate the color value of pixels [Piotr Dubla, "Interactive Global Illumination on the CPU."
				]}
			\label{fig:1}
		\end{center}
	\end{figure}

		
	\subsection{Raytracing definition}
	So what is Raytracing? Raytracing is a rendering technique that provides highly lifelike lighting effects. In other words, an algorithm can track the source of light and then mimic how the light interacts with the virtual objects it eventually encounters in the computer-generated environment. Raytracing produces far more lifelike shadows and reflections, as well as significantly enhanced translucence and dispersion. The algorithm considers where the light falls and calculates the interaction and interplay in the same way as the human eye does with actual light, shadows, and reflections.
	
	\vspace*{5px}
	
	\subsection{Raytracing mechanism}
	Raytracing follows three main steps: 
	\textit{Casting Rays}: This is similar to how the eye works, however rather than eyes works as a sensor, we have a camera, we shoot rays from the camera to the scene, and we calculate which is the closest object it hits, its color and brightness. This is done for each pixel.
	\textit{Path tracing}: Casting rays can solve the visibility issue; this means which object appears on the camera and at what position and color. However, if we want to simulate the following effects:  soft shadows, depth of field, motion blur, caustics, ambient occlusion, and indirect lighting, then we need to trace the rays from or to the light source, this will accumulate the brightness of an object or model in a scene, also will give the depth of different objects. Note that the more rays and depth we need, the more quality we get but the slow the simulation becomes due to the complexity.
	\textit{Shading}: In addition to tracing rays, we need a model to simulate different materials like Transparent, Glossy, and Diffuse. This is where the Shading phase is needed.
	
	\vspace*{5px}
	Raytracing has two main methods: 
	\begin{itemize}
		\item 		\textit{Forward Raytracing}: The light particles (photons) are tracked from the light source to the object via Forward Raytracing. While forward ray tracing is the most exact method for determining the color of each object, it is also the most inefficient.
		\item 		\textit{Backward Raytracing}: 
		An eye ray is formed at the eye in backward ray tracing, and it travels through the viewplane and out into the scene. If it hits an object, it will return it to the viewplane immediately. This method is more efficient than Forward Raytracing but less accurate due to reducing the rays used. In this implementation this method is used.
	\end{itemize}
	
	\section{Raytracer Implementation}
	In this chapter, raytracer terms and definitions are introduced, illustrating the implementation and decisions that have been made. 
	We need to set up our implementation to work for the raytracer as shown in Figure 2; the basic raytracer requires a camera; its responsibility is to shoot rays that travel through the scene and return a value or color. How many rays should it shoot? This depends on the width and height of the image we want to generate, the bigger the image, the more details we capture, but the more expensive computation gets. The rays idea is to find an intersection with the scene objects and try to return a color of the object to represent a corresponding pixel value. For object visibilities, we do not need a light source; however, to make the scene more realistic, we need to add shadows and other effects as reflections and refractions; hence we need a source light and tools, as shown in Figure 2.
	
	The algorithm works as follows, initiating the camera position and the orientation direction or where it looks at. Secondly, we subdivide the scene into pixels; this is based on the settings of the raytracer, usually by setting a height and width, reading a scene from usually an XML file that has a description of the number of objects in the scene, their color, and type. Afterward, generating rays for each pixel, this ray will be shooted toward the scene, and for each ray, we go through all the objects and test intersection tests; if the test returns true, we save its position, and we save the object, we do this for all objects and if more than two objects are intersected we compare between their position to detect which one is closer to the camera. In order to create shadows, we track the intersection point from the camera toward the light; if there is an object between the light source the hitting point, then we detect shadow. The corresponding value of the pixel is saved in a buffer and after the ray tracing is done, the pixels value are saved in a file of PPM extention.
	
	This is the basic idea, and to improve it, other topics need to be covered, such as Data structures, Objects Materials, Anti aliasing, Soft shadow. In this chapter, the basic blocks and components that make up the raytracer will be explained; only the methods are used will be explained, because as mentioned before, the performance of the raytracer is the main scope for this project.
	 
	 	 	
	 \begin{figure}[H]	
	 	\begin{center}
	 		\includegraphics[width=0.7\textwidth]{4.png}
	 		
	 		\caption{ An image showing how a ray is casted from the camera and how objects are illuminated and shadows are computed in ray tracing	.} %\protect\cite{haines2019ray}%
	 		\label{fig:1}
	 	\end{center}
	 \end{figure}
 
	\subsection{Software architecture}
	\subsubsection*{Configuration}
	We start with the configuration of the Raytracer by introducing a \textbf{struct Settings}, here we specify the following properties of the raytracer such as the \textbf{width} of the image, \textbf{height} of the image, the \textbf{background default color} of the scene, \textbf{maximum depth of path tracing},  \textbf{anti-aliasing samples} and \textbf{acceleration data Structure type}. Some of the previous terms will be explained later. 


	\subsubsection*{Scene generation}
	To create a scene, spheres will be used to create meshes; mesh is a collection of objects at a position and orientation to build a much more complex object. For example, the Stanford bunny shown in Figure 3 is composed of approximately 35947 spheres. Usually, meshes are made of triangles because they are more efficient and easier to build a mesh; however, I want to try to use something else and test the result and the quality of spheres. \textbf{createScene()} is implemented to load .obj files the next models will be used for testing in this report: \textbf{Stanford Bunny}, \textbf{Igea} and \textbf{Armadillo}, Figure 4 shows the different complex model that will be used for testing.

	 \begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{5.PNG}
	
	\caption{ Stanford bunny and a crown made of a collection od spheres	.}
	 %UnrealHaptics: Plugins for Advanced VR Interactions in Modern Game Engines%
	\label{fig:2}
	\end{center}
	\end{figure}

	\begin{figure}[ht]
	\begin{center}
		\begin{subfigure}{.3\textwidth}
			\centering
			% include first image
			\includegraphics[width=.8\linewidth]{armadillo.png}  
			\caption{Armadillo model}
			\label{fig:sub-first}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			% include second image
			\includegraphics[width=.8\linewidth]{igea.png}  
			\caption{Igea model}
			\label{fig:sub-second}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			% include second image
			\includegraphics[width=.8\linewidth]{stanford-bunny.png}  
			\caption{Stanford-bunny model}
			\label{fig:sub-third}
		\end{subfigure}
		
		\caption{Rendering different shapes in the scene}
		\label{fig:5}
		
	\end{center}
\end{figure}

    The \textbf{Sphere} is a class with the next attributes: \textbf{Center}: Position of the sphere, \textbf{MaterialType}: \textit{diffuse}, \textit{glossy}, \textit{reflection} and \textit{refraction} ,\textbf{Radius}: Sphere radius usually 1, \textbf{surfaceColor}: Color of the sphere in \textbf{RGB} format, and \textbf{emissionColor}: This is for the light.   
	Lights are just vector of spheres where the \textbf{emissionColor} is set to one. The light emeission is using the \textbf{inverse-square law}. In science, an inverse-square law is any scientific law stating that a specified physical quantity is inversely proportional to the square of the distance from the source of that physical quantity. In other words, the more significant the distance between the light source and the object, the less light it reaches the object surface.
	
		\noindent For point lights, this can be formulated as:
	\begin{equation}
		l_{ip} = \frac{l_{i}}{4 * \pi * d^2}
	\end{equation}
	where $l_{ip}$ is the actual intensity reaching a point $\boldsymbol{p}$, $l_{i}$ is the intensity value of the light and $d$ is the distance between the light position $\boldsymbol{l}_{p}$ and point $\boldsymbol{p}$.
	\subsubsection*{Shooting rays}
		For this, all raytracers use rays as a way to simulate photons. Let us think of a ray as a function; here $\pmb{p} $ is a 3D position along a line in 3D. $\pmb{o} $ is the ray origin and $\pmb{d} $ is the ray direction.
	\begin{equation}
		\pmb{p} (t) = \pmb{o} + t\pmb{d} ,  \quad  0 < t < \infty
	\end{equation}
	
	Figure 5 shows how Rays are used and how equations 1 can be used to check intersection tests and calculate the distance between the origin ray point, the camera, and the intersection point.  Point distance $\pmb{d} $ is used to compare the points to check which object is closer to the camera. 
	
			 \begin{figure}[H]	
			\begin{center}
				\includegraphics[width=0.7\textwidth]{6.png}
				
				\caption{ An image showing how a ray is casted from the camera and how objects are illuminated and shadows are computed in ray tracing	.} %\protect\cite{haines2019ray}%
				\label{fig:1}
			\end{center}
		\end{figure}
	
	\subsubsection*{Rendering Sphere}
	For testing, spheres are often used in ray tracers because
	calculating whether a ray hits a sphere is pretty straightforward.
	
	The general equation of a sphere with radius = 1 is:
	\begin{equation}
		(o_x + td_x )^2 + (o_y + td_y )^2+ (o_z + td_z )^2= 1
	\end{equation}
	
	For solving the equation we need to use the Quadratic equation in $ t $: 
	
	\begin{equation}
		\begin{split}
			A(t)^2 + Bt+ C= 0 \\
			A = d^2_x + d^2_y + d^2_z \\	
			B = 2(d_xo_x + d_yo_y + d_zo_z) \\
			C = o^2_x + o^2_y + o^2_z - 1 \\
			t_{1,2} = \frac{-B \pm \sqrt{B^2 - 4AC}}{2A}\\
		\end{split}		
	\end{equation}
	
	
	By solving the equation we get three different cases as shown in Figure ~\ref{fig:2}:
	
	\begin{itemize}
		\item No Intersection if: \( B^2 - 4AC < 0 \) 
		\item Single point of intersection if: \( B^2 - 4AC = 0 \)
		\item Otherwise we get two points of intersection
	\end{itemize}
	
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Raytracer/lab_report_1/assignment_2/images/5.png}
			
			\caption{The three possible line-sphere intersections:
				1. No intersection.
				2. Single point intersection.
				3. Two point intersection.}
			\label{fig:2}
		\end{center}
	\end{figure}
	
	\subsubsection*{Shading}
	The second step in rendering a scene is \textit{Shading}, and this deals with the color of the object and its intensity. Shading also includes how object's color affects each other; for example, having light hits, the object will make its color look brighter; on the other hand, regions in which light does not hit or reach will have dark color or shadow. In this chapter, shading concepts will be discussed and implemented, in addition to different materials that have different properties and how they interact with the light. The primary key to Shading is calculating the amount of light that hits a point; let us call it $ P $. 
	
	The computed light at a point $ P $ depends on the following: 
	
	\begin{itemize}
		\item Light illuminated by source  $\pmb{L}^{source}$  in real life usually lamp, fire or the sun, it can have any color and intensity but here we will use white color. 
		\item Surface illumination $\pmb{L}^{surface}$.
		\item Light reflected from the surface $\pmb{L}^{reflected}$.
		\item The observation angle / looking at angle / camera. 
	\end{itemize}
	
	
	
	
	\subsubsection{Lambert's Cosine Law}
	The amount of light energy arriving at a surface is proportional to the cosine of the angle between the light direction and the surface normal, according to \textit{Lambert's cosine law}. Illumination strength at a surface is proportional to the cosine of the angle between $\pmb{l}$ and $\pmb{n}$, the angel will be denoted as $\theta$, the following three cases illustrate the relationship between the  $\pmb{L}^{source}$ and  $\pmb{L}^{surface}$:
	
	The  $\pmb{L}^{surface}$,$ \pmb{L}^{source}$ relation is:
	
	\begin{equation}
		\pmb{L}^{surface} = \pmb{L}^{source}.\cos \theta 
	\end{equation}
	
	
	\begin{itemize}
		\item $\pmb{L}^{surface} = \pmb{L}^{source}$, if $\theta = 0\degree$.
		\item $\pmb{L}^{surface} = 0$, if $\theta = 90\degree$.
		\item $0 < \pmb{L}^{surface} < \pmb{L}^{source}$, if $0\degree < \theta  < 90\degree$.
	\end{itemize}
	
	\subsubsection{Phong reflection model }
	Phong reflection is a model of local illumination. It defines how light reflects off a surface as a mixture of \textit{diffuse} reflection from rough surfaces and \textit{specular} reflection from polished surfaces. It's based on Phong's intuitive observation that bright surfaces have small, strong specular highlights, and dull surfaces have larger, more gradual specular highlights. The model also includes an ambient term to account for the small amount of light that is scattered about the entire scene.
	\\
	
	\begin{itemize}
		\item \pmb{Ambient reflection}
		\begin{equation}
			\pmb{L}^{amb} = \pmb{\rho}\otimes \pmb{L}^{indirect}
		\end{equation}
		\left 
		\begin{itemize}
			\item $\pmb{\rho}$, is the surface color
			\item $\pmb{L}^{indirect}$ , is the light reflected from other surfaces and objects, excluded the direct light ($\pmb{L}^{source}$)
		\end{itemize}
		\item \pmb{Diffuse reflection} 
		
		\begin{equation}
			\pmb{L}^{diff} =  \pmb{L}^{source}.(\pmb{n}.\pmb{l}) \otimes \pmb{\rho} 
		\end{equation}
		\begin{itemize}
			\item $\pmb{L}^{source}$, is the light source color and intensity which usually white. 
			\item $\pmb{n}$ and $\pmb{l}$ , are the representation of the Lambert's cosine law, where $\pmb{n}$ is the normal surface vector  and $\pmb{l}$ is the indecent light coming from the light source.
		\end{itemize}
		\item \pmb{Specular reflection} 
		
		\begin{equation}
			\pmb{L}^{spec} =  \pmb{L}^{source}.(\pmb{n}.\pmb{l}).(\pmb{r}.\pmb{v})^m \otimes \pmb{\rho}^{white} 
		\end{equation}
		\begin{itemize}
			\item $\pmb{r}$, which is the direction that a perfectly reflected ray of light would take from this point on the surface. 
			\item $\pmb{v}$, which is the direction pointing towards the viewer (such as a virtual camera).
			\item $m$, which is a shininess constant for this material, which is larger for surfaces that are smoother and more mirror-like. When this constant is large the specular highlight is small.
		\end{itemize}
	\end{itemize}
	
	The overall illumination on the surface can be computed by summing up the three components that make up \textit{Phong model}:
	
	\begin{equation}
		\pmb{L}^{surface}= \pmb{L}^{amb} + \sum_{n=1}^{lights} (\pmb{L}_n^{diff} + \pmb{L}_n^{spec})
	\end{equation}
	
	
	\begin{figure}[ht]
		\begin{center}
			\begin{subfigure}{.3\textwidth}
				\centering
				% include first image
				\includegraphics[width=.8\linewidth]{C:/D/University/Semester_4/Raytracer/lab_report_1/assignment_2/images/amb_reflection.png}  
				\caption{Ambient $ \pmb{L}^{amb} $}
				\label{fig:sub-first}
			\end{subfigure}
			\begin{subfigure}{.3\textwidth}
				\centering
				% include second image
				\includegraphics[width=.8\linewidth]{C:/D/University/Semester_4/Raytracer/lab_report_1/assignment_2/images/diff_reflection_2.png}  
				\caption{Diffuse $ \pmb{L}^{diff} $}
				\label{fig:sub-second}
			\end{subfigure}
			\begin{subfigure}{.3\textwidth}
				\centering
				% include second image
				\includegraphics[width=.8\linewidth]{C:/D/University/Semester_4/Raytracer/lab_report_1/assignment_2/images/spec_reflection_2.png}  
				\caption{ Specular (Glossy)  $ \pmb{L}^{spec} $}
				\label{fig:sub-third}
			\end{subfigure}
			
			\caption{Visual illustration of the Phong equation}
			\label{fig:6}
		\end{center}
	\end{figure}


	\section{Performance}
	This is the most exciting part of the project; the previous chapters explained how to set up the current Raytracer and what methods have been used to implement it; however, the result is not shown yet. This chapter will apply different performance tests on the Raytracer to show its average performance without optimizing the datastrcuter. 
	
	In the renderer the next setup is used: 
	
	\begin{table}[h]
		\centering
		\begin{tabular}{ c|c  }
			\thickhline%\toprule
			Key   &   Value \\
			\thickhline%\toprule
			Height   &   480 \\
			
			
			Width   & 640 \\
			
			
			DataStructure   & None \\
			
			
			Anti-aliasing samples  & 2 \\
			
			
			PC info  & 	Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz   1.99 GHz  RAM	16.0 GB\\
			\thickhline%\toprule
		\end{tabular}
				\caption{The Raytracer settings.}
	\label{table:2}
	\end{table}
 
 	Three different simulations were used to test the performance and took their average for each scenario or model, as shown in Table 2. The more simulations are run, the more accurate results we get; however, three simulations are used to test the performance because each takes approximately one hour.
 	As discussed before, performance is the main discussion point in this project rather than the accuracy of the renderer; hence the spent amount of time to render should be discussed and optimized. 
 	In order to renderer more than three thousand spheres, as shown in Table 2, this can take up to one hour; this is not optimal. One model only as the Bunny needs one hour to be rendered is not practical in real-life simulation applications.  
 	We need to find the most expensive block code the Raytracer spends most of its time optimizing the rendering. The used Raytracer can be divided into the following phases: 
 	
 	\begin{itemize}
 		\item \textbf{Creating a scene}: For this part \textbf{\textit{createScene()}} method is responsible for creating the scene, this includes loading primitives from a .obj file, this steps is governed by only the number ob premieres to be created this means it is worth optimizing it, because id depend on how big the scene is.   It takes < 1 minutes from the overall rendering time, which is < 1\% of the total time, hence this step is not so dominant.
 		\item  \textbf{Ray casting}: This part usually contains two for loops to go over through all the pixels in the screen, which depends on the width and height of the image to be rendered. The smaller the size of the image, the faster the rendered become as the for loop gets smaller; however, this is not helpful as we want to have a high-quality image and not lose pixels.
 		The time complexity of the basic algorithm is $ O(w hr) $, where $ w $ and $ h $ are the width and the height of the display screen in pixels; and $ r $ is the complexity of shooting a ray into the scene. (refrence to Accelerating algorithms for Ray Tracing)
 		 For each pixel, there is a ray to be cast to the scene, and then the next two sub-steps are applied: 
 			\begin{itemize}
 			 \item \textbf{Intersection tests}: Most objects in the scene are composed of a combination of basic shaped objects like spheres or triangles; these basic objects are connected in a mesh to build bigger complicated objects, and to render the model, each object has to be tested against the shooted ray, to test if it is visible to this ray or not. For each object, there is \textbf{\textit{isIntersected}} function used to test the intersection. This function itself is not huge, but repeating it n times can be time-consuming.
 			\item  \textbf{Evaluate pixel color}: Evaluating the pixel's brightness and if it is transparent or reflects light, all these steps are done after the intersection test returns true, because if the rays intersect the surface object, we want to evaluate it is proper to color, not only the diffuse color. This needs a recursion operation to cast more and more rays into the scene, and this depends on the settings of the raytracer as the parameter usually called depth $ d $.
 			\end{itemize}
 		
 		This concludes that the worst case time complexity for shooting a ray is $ O(nd) $, where $ n $ is the number of spheres and $ d $ is the maximum level of recursion established.
 		For a non-stochastic raytracer, $ d $ is constant and given by the user; hence it is not interesting for now; hence with a fixed width, height, and depth, a naive raytracer depends on the number of objects in the scene only $ n $. 
 		\item \textbf{Write pixels values into a file}: This step is $ O(wh) $ as it depends only on the number of pixels to be writen into a file, note that PPM format is used in this raytracer.
 		 	\end{itemize}
 		I hope by now it has been proven that the most expensive part in the raytracer is the number of intersection tests and how many recursion made to evaluate the color of the pixel, for the second part most modern raytracers uses a stochastic method such as Monte Carlo, but to reduce the number of intersection tests a special acceleration methods are used.  
 		   
 		In this project two different methods are used to reduce the number of intersection tests:
 		
 		 	\begin{itemize}
 			\item 
 		\textbf{Bounding hierarchies}. This method is object focused where it uses the Bounding volume to encapsulate each object in the scene with a bounding volume, in addition, to join each neighbor bounding volume to create a parent node that bound both children. This method reduces the complexity from $  O(w h n) $ to$  O(w h m) $, where m < n is the number of BV nodes. In the next chapters, we will discuss the advantages and disadvantages of this method. [Refrence to Accelerating algorithms for Ray Tracing].
 		 
 		\item  \textbf{Spatial coherence}.
 		This method is space-focused, where it divides the space into regions; only the objects inside the region that intersected with the casted ray are tested; this reduces the complexity to $ O(w h \log  n) $ this method also needs a preprocessing step to build a tree. More details are discussed in the following chapters. 
 		
 		 [Refrence to Accelerating algorithms for Ray Tracing].
 		 	\end{itemize}

 
		\begin{table}[H]
		\centering
		\begin{tabular}{>{\centering\arraybackslash}m{.1\linewidth}|>{\centering\arraybackslash}m{.1\linewidth}|>{\centering\arraybackslash}m{.1\linewidth}|>{\centering\arraybackslash}m{.1\linewidth}|>{\centering\arraybackslash}m{.6\linewidth}}
			\thickhline%\toprule
			   Model   &   Spheres  & Sphere intersection tests  &   Time taken to render  & Result \\
						\thickhline%\toprule
			     Bunny   &
			   35947  & 1841.98m &   104.31 minutes  & \includegraphics[width=0.3\textwidth, height=50mm]{stanford-bunny_re.PNG}\\
			
			 Armadillo  &   49990    &1822.94m&   45.34 minutes &    \includegraphics[width=0.3\textwidth, height=50mm]{armadillo_re.PNG} \\
			
			 Igea   &  134345    &7278.88m&   187.5 minutes & \includegraphics[width=0.3\textwidth, height=50mm]{igea_re.PNG} \\
						\thickhline%\toprule
		\end{tabular}
			\caption{Performance comparison for different scenarios on 480X640.}
	\label{table:2}
	\end{table}
	\clearpage
	
	
	\section{Object subdivision}
	
		\subsection{Introduction}
	\label{definitions}
	Bounding Volume Hierarchies, known as "BVH," is simply a data structure representing complex geometric models with specific simple bounding volumes to reduce some of the expensive tests in different computer graphics applications.
	BVH is object-oriented, unlike other algorithms such as kd-trees which are space subdivisions.
	
	
	
	
	\subsection{Motivation}
	There are two main applications for bounding volume hierarchy, \textit{Retracing} where two main challenges have to be solved: \textit{shading} and \textit{visibility}; usually, the Raytracer has to test each model in the scene in order to render it to the screen, on the other hand, some of the models they are not visible to the camera and testing them make no sense therefore by informing the rays which models to test the intersection this will boost the performance of the Raytracer tremendously, BVH can quickly achieve this.
	
	Moreover, the \textit{collision detection} algorithm used in simulation and games can benefit significantly from BVH. In \textit{fluid simulation} for example, one scene can have thousands of particles that act as a fluid; these particle change their position in each frame, hence for each frame, these particles have to go through the collision detection test. However, not all particles are close to each other, and one does not have to test each particle against the others; hence BVH can be used to detect a potential collision. 
	
	
	
	
	\clearpage
	%----------------------------------------------------------------------------------------
	%	SECTION 2
	%----------------------------------------------------------------------------------------
	
	\section{Bounding volumes BV}
	\subsection{Concept}
	Bounding volume is the tightest possible virtual volume that wraps up a model in a scene. It is the primary component that builds the BVH tree.
	
	There are four different main types of BV depending on the shape complexity:
	
	\begin{itemize}
		\item Spheres.
		\item Axis Aligned Bounding Box (AABB).
		\item Oriented Bounding Box (OBB).
		\item Discrete Oriented Polytope (k-DOP).
		
	\end{itemize}
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Seminar/images_seminar/bvs.png}
			
			\caption{Bounding volumes types [Suaib et al., 2008, Bade et al.,2006]}
			\label{fig:1}
		\end{center}
	\end{figure}
	
	
	In Figure ~\ref{fig:1}, the more right we go, the more complex the shape becomes; on the other hand, the fewer overlap tests required; however, going to the opposite side from right to left, the simpler shape becomes; hence, the faster the overlap test is, hence choosing the right BV is an important parameter in the BVH, this depends on the application and use case as there is no perfect BV that dominates the rest.
	
	\subsection{Choosing the right BV}
	Choosing the right BV can be challenging because it depends on the spread of the primitive.
	Figure ~\ref{fig:2} shows some different random scenarios that can happen in a scene, looking at the Sphere at the first row, by taking the left column, where the primitives are uniformly distributed in the form of a circle, hence encapsulating them by a sphere BV can be optimal; nevertheless, the right column shows a lousy choice to wrap a collection of primitives that makes a rectangular shape object by a sphere, where the BV is not tight enough. 
	For AABB, choosing it to encapsulate a model that looks like a square can be a good choice, as shown in the left column; however, if the box is oriented as the right column, then it is a wrong choice to work with AABB. 
	Turning to OBB bounding volume where it fits perfectly into the oriented box that both Sphere and AABB failed to achieve because it takes the orientation into count, nonetheless, OBB can perform poorly on some cases as the right column illustrates, the same rectangular but with four outliers primitives that change the orientation of the box and makes it not tight. 
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Seminar/images_seminar/bv_choose.png}
			
			\caption{Different Bounding volumes try to fit perfectly to random primitives [Suaib et al., 2008, Bade et al.,2006]}
			\label{fig:2}
		\end{center}
	\end{figure}
	\begin{verbatim}
		
	\end{verbatim}
	
	
	\subsection{OBB fitting}
	In order to generate and create an OBB box, three main parameters are needed: \textit{Orientation}, \textit{Center}, and \textit{Minimal edge length}.
	To calculate the orientation we use covariance matrix:
	
	\begin{equation}
		\textbf{C}_ij = \frac{1}{n}\sum_{i=1}^{n} (\textbf{x}_i - m)_j(\textbf{x}_i - m)_k 
	\end{equation}
	\begin{equation}
		m = \frac{1}{n}\sum_{i=1}^{n} (\textbf{x}_i)
	\end{equation}
	
	
	
	
	Choosing the right primitives to use for the covariance matrix is critical; there are three ways to choose the primitives: 	
	
	\begin{itemize}
		\item \textit{Maximal spread of primitives}: This includes the whole primitives into the calculation.
		\item \textit{Extremal primitives}: this includes only the external primitives. 
		\item \textit{Convex hull boundary}: Creating a convex hull boundary and trying to fit the OBB to it, this method is arguably more robust than the previous methods, but it has a complexity of $\textbf{\textit{O}}(nlogn)$.
	\end{itemize} 
	
	Fitting an OBB into a square can be tricky because squares have multiple orientations, and the worst case when the BV takes double the size of the actual square, nevertheless, based on [Stefan Gottschalk. "Collision Queries using Bounding Box"] paper, a test has been done for fitting OBB to a square 10,000 times, show its common to have factors of 1.0 and 1.1  but not 2.0; therefore, this case is rare. 
	
	\subsection{OBB Overlap test}
	
	Assuming we have two OBB boxes and we want to test if they overlap or not, a distance $T$ between their centroids must be calculated, then projected into axis $n$ to calculate $s$ as follow:
	\[ s = \textbf{T}.\textbf{n}  \]
	
	Calculating the half width of both boxes $r_a$ and $r_b$ to execute the overlap test: 
	\[ s > r_a + r_b  \]
	
	If this condition is true, then there is no overlap between the OBB boxes.
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Seminar/images_seminar/overlap.png}
			
			\caption{OBB Overlap test illustration [Stefan Gottschalk. “Collision Queries using Bounding Box”]}
			\label{fig:boat1}
		\end{center}
	\end{figure}
	
	
	\clearpage
	
	%----------------------------------------------------------------------------------------
	%	SECTION 3
	%----------------------------------------------------------------------------------------
	\section{Bounding Volume Hierarchies BVH}
	\subsection{Tree construction}
	Nodes in the BVH tree are BV, and leaves are primitives. 
	
	Tree characteristic for an optimal tree:
	\begin{itemize}
		\item Simple BV.
		\item Tight BV.
		\item Depth of the tree.
		\item Balance tree.
		\item Minimal overlap. 
	\end{itemize} 
	
	There are three different ways to construct a BVH tree:
	\begin{itemize}
		\item \textit{Top Down}:	Arguably the most popular technique in practice. It uses the '\textit{fit and split}' algorithm, where it starts with the whole model and encapsulates it with a BV and fits it, then tries to split it into $ n $ children, usually 2. It keeps recursively splitting and fitting until it reaches the leaves and assigns the primitives to them.
		
		\item \textit{Bottom Up}: Slower construction time than Top-Down but usually produces the best tree. It uses the '\textit{knit and fit}' algorithm, where it starts with primitives and tries to encapsulate them with the BV and merge each $ n $ together, usually 2; it keeps recursively doing this process until it reaches the root.
		
		
		\item \textit{Insertion}: It uses '\textit{incremental-insertion}' algorithm, where it starts with a single leaf and merge to it another leaf by using a cost function, then creating a head node and search for the next leaf to merge. The problem with this method is that it can become worst as it depends on the insertion order of the nodes, and it is challenging to find the best tree.
		
	\end{itemize} 
	
	
	
	There are three different partitioning strategies and splitting algorithms:
	\begin{itemize}
		\item \textit{Median of the centroid coordinates} (Object median): Resulting in a well-balanced tree.
		\item \textit{Mean of centroid coordinates} (Object mean): Gives smaller volume trees.
		\item \textit{Spatial median}: Splitting the volume into two equal parts.
		
	\end{itemize} 
	
	
	\subsection{Tree update}
	Updating is necessary for each time step due to movement/deformation of the simulated object; the naive solution is to recreate the BVH tree for each new frame; however, the more efficient method is to refine the BVH tree from the previous frame. There are different algorithms for updating the tree, and one of them is \textit{Incremental Updates via Tree Rotations}.
	
	Figure ~\ref{fig:4} illustrates a scene composed of three primitives, a, b and c. After creating the first BVH tree, the next frame shows that c moves from top to bottom to create a wrong BVH tree, and the algorithm solves this by using rotation were to replace c with a, and this makes a better BVH again. 
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Seminar/images_seminar/update.png}
			
			\caption{Updating BVH tree [Daniel K, Thiago I, Josef S, Erik B, Al D, Andrew K, “Fast, Effective BVH Updates for Animated Scenes.].}
			\label{fig:4}
		\end{center}
	\end{figure}
	
	
	
	\subsection{Tree traversal: Collision detection algorithm}
	The collision detection algorithm is one of the common tests in Simulation applications, and enhancing its performance can be done by adding the BVH tree to its models. 
	Algorithm ~\ref{alg:1} shows the pseudo-code for implementing such a test by using the BVH tree; note that to accelerate the test, one can jump from one layer chosen by the settings to the leaf directly; this is called the "Fast collision detection method."
	
	\begin{algorithm}[H]
		\caption{Collision detection algorithm}\label{euclid}
		\begin{algorithmic}[1]
			\Procedure{MyProcedure}{}
			\State $\textit{Beginning at the root nodes of two given trees}$
			\If {$ \textit{root nodes do not intersect} $} \Return No collision
			\Else
			\State {$ \textit{check all children of one node against all Children of the other node} $}
			\If {$ \textit{there is intersection between any children} $}
			\If {$ \textit{leaf node} $} \Return  Collision
			\EndIf
			\Else		
			\State \textbf{goto} \emph{4}.
			\EndIf
			\EndIf
			
			\If {$ \textit{No children left} $} \Return No collision
			\EndIf
			\EndProcedure
		\end{algorithmic}
		\label{alg:1}
	\end{algorithm}
	
	\clearpage
	\section{Performance}
	Comparing two different data accelerators can be beneficial; it gives a sense of what are these data structures capable of. Figure ~\ref{fig:5} shows the performance of kd-tree and BVH in different scenes, and the x-axis shows how complex the scene is. The left you go, the more complex the scene gets; the y-axis shows the complexity of the time consumed to perform ray intersection. By looking at the graph, we notice that kd-tree outperforms BVH in the complex scenes on the left; however, this is not the case on the simpler scenes on the right. We should notice that this is not always true as it depends on the algorithms that have been used between the different data structures as in the BVH they are three approaches in order to construct and also to update the tree and also to detect the intersection, also changing the BV type could dramatically change the performance of the BVH tree as always depends on the application.
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=250pt]{C:/D/University/Semester_4/Seminar/images_seminar/performance.png}
			
			\caption{Comparison between BVH and Kd-trees [Marek V, Vlastimil H, Jiri B. Performance comparison of Bounding Volume Hierarchies and Kd-Trees for GPU Ray Tracing]}
			\label{fig:5}
		\end{center}
	\end{figure}
	
	\clearpage
	\section{Summary}
	\begin{itemize}
		
		
		\item BVH is a great method to boost the raytracing and collision detection operations. 
		\item BVH depends highly on the BV type.
		\item BV is a simple geometry that encapsulates a complex one.
		\item Fitting BV is a challenge.
		\item BV Overlap tests are a challenge.
		\item BVH has different construction algorithms and settings. 
		\item How to update BVH tree: Incremental Updates via Tree Rotations.
		\item How to detect collision by using BVH tree. 
		\item BVH is a good alternative to the kd-tree. 
		
		
	\end{itemize} 
	\clearpage
	\section{Spatial Subdivision}
	
	This lab was a great way to gain practical experience of the concepts we learned in our computer graphics course. I would like to thank \textbf{Prof. Dr.-Ing. Matthias Teschner} again for advising me throughout this lab. As a final remark, I would like to mention that I really enjoyed working on area lights. This includes debugging issues as I was working on them as well as how they were fixed. At the end, the results were worth the effort.
	
	\bibliographystyle{apacite}
	Accelerating algorithms for Ray Tracing, H´ector Antonio Villa Mart´ınez
	
					\bibitem{latexcompanion} 
	Daniel K, Thiago I, Josef S, Erik B, Al D, Andrew K, 	\textit{"Fast, Effective BVH Updates for Animated Scenes"}. University of Utah and Pixar
	
	\bibitem{latexcompanion} 
	Fangkai Y. 	\textit{"Collision Detection in Computer Games"}. KTH Royal Institute of Technology
	
	\bibitem{latexcompanion} 
	Hamzah S, Abdullah B. 	\textit{"Bounding. Volume Hierarchies for Collision Detection"}
	
	\bibitem{latexcompanion} 
	Jacco B. 	\textit{"Advanced Graphics - Acceleration Structures"}. Utrecht University
	
	\bibitem{latexcompanion} 
	Matthias Teschner, 	\textit{“Simulation in Computer Graphics Bounding Volume Hierarchies”}, University of Freiburg
	
	\bibitem{latexcompanion} 
	Marek V, Vlastimil H, Jiri B .	\textit{"Performance Comparison of Bounding Volume Hierarchies and Kd-Trees for GPU Ray Tracing"}. Masaryk University and Czech Technical University in Prague
	
	\bibitem{latexcompanion} 
	Stefan G.	\textit{"Collision queries using bounding box"}. The University of North Carolina
	
	\bibitem{latexcompanion} 
	Yingsong H, Weijian W, Dan L, Qingzhi Z, Yunfei H. \textit{"Parallel BVH Construction Using Locally-Density Clustering"}. School of Computer Science and Technology, Huazhong University of Science and Technology
	
	\bibliography{References}
	
\end{document}